#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸œæ–¹è´¢å¯Œè‚¡ç¥¨èµ„è®¯ã€å…¬å‘Šã€ç ”æŠ¥çˆ¬å–å’Œæƒ…æ„Ÿåˆ†æè„šæœ¬ - ä½¿ç”¨APIæ¥å£ç‰ˆæœ¬
"""

import sys
import json
import time
import re
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import requests
from bs4 import BeautifulSoup
import os

class EastMoneyStockNews:
    def __init__(self, deepseek_api_key: str):
        """
        åˆå§‹åŒ–è‚¡ç¥¨èµ„è®¯çˆ¬å–å™¨
        
        Args:
            deepseek_api_key: DeepSeek APIå¯†é’¥
        """
        self.deepseek_api_key = deepseek_api_key
        self.deepseek_url = "https://api.deepseek.com/v1/chat/completions"
        self.deepseek_model = "deepseek-chat"  # DeepSeek V3
        
        # ç”¨æˆ·ä»£ç†æ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.0.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # è¯·æ±‚å¤´æ± 
        self.headers_pool = [
            {
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Referer': 'https://data.eastmoney.com/',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            },
            {
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Referer': 'https://data.eastmoney.com/',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
            }
        ]
    
    def _get_random_user_agent(self) -> str:
        """è·å–éšæœºç”¨æˆ·ä»£ç†"""
        return random.choice(self.user_agents)
    
    def _get_random_headers(self) -> Dict[str, str]:
        """è·å–éšæœºè¯·æ±‚å¤´"""
        headers = random.choice(self.headers_pool).copy()
        headers['User-Agent'] = self._get_random_user_agent()
        return headers
        
    def get_stock_news_data(self, stock_code: str) -> Dict:
        """
        é€šè¿‡APIæ¥å£è·å–è‚¡ç¥¨çš„èµ„è®¯ã€å…¬å‘Šã€ç ”æŠ¥æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
             åŒ…å«èµ„è®¯ã€å…¬å‘Šã€ç ”æŠ¥çš„å­—å…¸
        """
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                print(f"å°è¯•è·å–è‚¡ç¥¨èµ„è®¯æ•°æ®ï¼Œç¬¬ {attempt + 1} æ¬¡...", file=sys.stderr)
                
                # è·å–èµ„è®¯æ•°æ®
                print("æ­£åœ¨è·å–èµ„è®¯æ•°æ®...", file=sys.stderr)
                news_data = self._get_news_from_api(stock_code)
                
                # è·å–å…¬å‘Šæ•°æ®
                print("æ­£åœ¨è·å–å…¬å‘Šæ•°æ®...", file=sys.stderr)
                notices_data = self._get_notices_from_api(stock_code)
                
                # è·å–ç ”æŠ¥æ•°æ®
                print("æ­£åœ¨è·å–ç ”æŠ¥æ•°æ®...", file=sys.stderr)
                reports_data = self._get_reports_from_api(stock_code)
                
                # åˆå¹¶ç»“æœ
                result = {
                    'stockCode': stock_code,
                    'timestamp': int(time.time() * 1000),
                    'news': news_data,
                    'notices': notices_data,
                    'reports': reports_data
                }
                
                # éªŒè¯æ˜¯å¦è·å–åˆ°æ•°æ®
                total_items = len(result['news']) + len(result['notices']) + len(result['reports'])
                if total_items == 0:
                    raise Exception("æ‰€æœ‰APIéƒ½æœªèƒ½è·å–åˆ°æ•°æ®")
                
                print(f"âœ… æˆåŠŸè·å–æ•°æ®: {len(result['news'])} æ¡èµ„è®¯, {len(result['notices'])} æ¡å…¬å‘Š, {len(result['reports'])} æ¡ç ”æŠ¥", file=sys.stderr)
                return result
                    
            except Exception as e:
                print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}", file=sys.stderr)
                
                if attempt < max_retries - 1:
                    print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...", file=sys.stderr)
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                else:
                    print(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ— æ³•è·å–è‚¡ç¥¨èµ„è®¯æ•°æ®", file=sys.stderr)
                    return None
        
        return None
    
    def _get_news_from_api(self, stock_code: str) -> List[Dict]:
        """é€šè¿‡APIè·å–èµ„è®¯æ•°æ®"""
        try:
            # èµ„è®¯APIæ¥å£
            url = "https://np-listapi.eastmoney.com/comm/web/getListInfo"
            params = {
                'client': 'web',
                'biz': 'web_voice',
                'mTypeAndCode': f'0.{stock_code}',  # æ ¼å¼ï¼š0.002241
                'pageSize': 50,  # å¢åŠ é¡µé¢å¤§å°
                'type': 1,
                'req_trace': self._generate_trace_id()
            }
            
            print(f"ğŸ” èµ„è®¯APIè¯·æ±‚å‚æ•°: {params}", file=sys.stderr)
            
            headers = self._get_random_headers()
            
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            print(f"ğŸ” èµ„è®¯APIå“åº”: {data}", file=sys.stderr)
            
            if data.get('code') == 1 and 'data' in data:
                if data['data'] is None:
                    print("âš ï¸ èµ„è®¯APIè¿”å›dataä¸ºNoneï¼Œå¯èƒ½è¯¥è‚¡ç¥¨æš‚æ— èµ„è®¯æ•°æ®", file=sys.stderr)
                    return []
                
                if 'list' in data['data'] and data['data']['list']:
                    news_list = []
                    for item in data['data']['list']:
                        news_list.append({
                            'title': item.get('Art_Title', ''),
                            'link': item.get('Art_Url', ''),
                            'date': item.get('Art_ShowTime', '')[:10] if item.get('Art_ShowTime') else '',
                            'timeStr': item.get('Art_ShowTime', ''),
                            'artCode': item.get('Art_Code', ''),
                            'source': item.get('Np_dst', '')
                        })
                    
                    print(f"âœ… æˆåŠŸè·å– {len(news_list)} æ¡èµ„è®¯", file=sys.stderr)
                    return news_list
                else:
                    print("âš ï¸ èµ„è®¯APIè¿”å›çš„listä¸ºç©º", file=sys.stderr)
                    return []
            else:
                print(f"âš ï¸ èµ„è®¯APIè¿”å›å¼‚å¸¸: {data}", file=sys.stderr)
                return []
                
        except Exception as e:
            print(f"è·å–èµ„è®¯æ•°æ®å¤±è´¥: {e}", file=sys.stderr)
            return []
    
    def _get_notices_from_api(self, stock_code: str) -> List[Dict]:
        """é€šè¿‡APIè·å–å…¬å‘Šæ•°æ®"""
        try:
            # å…¬å‘ŠAPIæ¥å£
            url = "https://np-anotice-stock.eastmoney.com/api/security/ann"
            params = {
                'sr': -1,
                'page_size': 50,  # å¢åŠ é¡µé¢å¤§å°
                'page_index': 1,
                'ann_type': 'A',
                'client_source': 'web',
                'stock_list': stock_code,
                'f_node': 0,
                's_node': 0
            }
            
            headers = self._get_random_headers()
            
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('success') == 1 and 'data' in data and 'list' in data['data']:
                notices_list = []
                for item in data['data']['list']:
                    notices_list.append({
                        'title': item.get('title', ''),
                        'link': f"https://np-anotice-stock.eastmoney.com/api/security/ann?art_code={item.get('art_code', '')}",
                        'date': item.get('notice_date', '')[:10] if item.get('notice_date') else '',
                        'timeStr': item.get('display_time', ''),
                        'artCode': item.get('art_code', ''),
                        'columnName': item.get('columns', [{}])[0].get('column_name', '') if item.get('columns') else ''
                    })
                
                print(f"âœ… æˆåŠŸè·å– {len(notices_list)} æ¡å…¬å‘Š", file=sys.stderr)
                return notices_list
            else:
                print(f"âš ï¸ å…¬å‘ŠAPIè¿”å›å¼‚å¸¸: {data}", file=sys.stderr)
                return []
                
        except Exception as e:
            print(f"è·å–å…¬å‘Šæ•°æ®å¤±è´¥: {e}", file=sys.stderr)
            return []
    
    def _get_reports_from_api(self, stock_code: str) -> List[Dict]:
        """é€šè¿‡APIè·å–ç ”æŠ¥æ•°æ®"""
        try:
            # ç ”æŠ¥APIæ¥å£
            url = "https://reportapi.eastmoney.com/report/list"
            
            # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘ä¸€å¹´ï¼‰
            end_time = datetime.now()
            begin_time = end_time - timedelta(days=365)
            
            params = {
                'pageNo': 1,
                'pageSize': 50,  # å¢åŠ é¡µé¢å¤§å°
                'code': stock_code,
                'industryCode': '*',
                'industry': '*',
                'rating': '*',
                'ratingchange': '*',
                'beginTime': begin_time.strftime('%Y-%m-%d'),
                'endTime': end_time.strftime('%Y-%m-%d'),
                'fields': '',
                'qType': 0,
                'sort': 'publishDate,desc'
            }
            
            headers = self._get_random_headers()
            
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if 'data' in data and isinstance(data['data'], list):
                reports_list = []
                for item in data['data']:
                    reports_list.append({
                        'title': item.get('title', ''),
                        'link': f"https://reportapi.eastmoney.com/report/detail?infoCode={item.get('infoCode', '')}",
                        'date': item.get('publishDate', '')[:10] if item.get('publishDate') else '',
                        'timeStr': item.get('publishDate', ''),
                        'orgName': item.get('orgSName', ''),
                        'rating': item.get('emRatingName', ''),
                        'researcher': item.get('researcher', ''),
                        'targetPrice': item.get('indvAimPriceT', '') or item.get('indvAimPriceL', '')
                    })
                
                print(f"âœ… æˆåŠŸè·å– {len(reports_list)} æ¡ç ”æŠ¥", file=sys.stderr)
                return reports_list
            else:
                print(f"âš ï¸ ç ”æŠ¥APIè¿”å›å¼‚å¸¸: {data}", file=sys.stderr)
                return []
                
        except Exception as e:
            print(f"è·å–ç ”æŠ¥æ•°æ®å¤±è´¥: {e}", file=sys.stderr)
            return []
    
    def _generate_trace_id(self) -> str:
        """ç”Ÿæˆè¯·æ±‚è¿½è¸ªID"""
        import hashlib
        timestamp = str(int(time.time() * 1000))
        random_str = str(random.randint(1000, 9999))
        combined = timestamp + random_str
        return hashlib.md5(combined.encode()).hexdigest()
    
    def filter_recent_week(self, data: Dict) -> Dict:
        """
        è¿‡æ»¤å‡ºæœ€è¿‘ä¸€å‘¨çš„æ•°æ®
        
        Args:
            data: åŸå§‹æ•°æ®
            
        Returns:
             è¿‡æ»¤åçš„æ•°æ®
        """
        try:
            # è®¡ç®—ä¸€å‘¨å‰çš„æ—¥æœŸ
            week_ago = datetime.now() - timedelta(days=7)
            week_ago_str = week_ago.strftime('%Y-%m-%d')
            
            filtered_data = {
                'stockCode': data['stockCode'],
                'timestamp': data['timestamp'],
                'news': [],
                'notices': [],
                'reports': []
            }
            
            # è¿‡æ»¤èµ„è®¯
            for news in data.get('news', []):
                if news.get('date') and news['date'] >= week_ago_str:
                    filtered_data['news'].append(news)
            
            # è¿‡æ»¤å…¬å‘Š
            for notice in data.get('notices', []):
                if notice.get('date') and notice['date'] >= week_ago_str:
                    filtered_data['notices'].append(notice)
            
            # è¿‡æ»¤ç ”æŠ¥
            for report in data.get('reports', []):
                if report.get('date') and report['date'] >= week_ago_str:
                    filtered_data['reports'].append(report)
            
            return filtered_data
            
        except Exception as e:
            print(f"è¿‡æ»¤æœ€è¿‘ä¸€å‘¨æ•°æ®å¤±è´¥: {e}", file=sys.stderr)
            return data
    
    def get_news_content(self, url: str) -> str:
        """
        è·å–æ–°é—»å†…å®¹
        
        Args:
            url: æ–°é—»é“¾æ¥
            
        Returns:
             æ–°é—»å†…å®¹
        """
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"é‡è¯•è·å–æ–°é—»å†…å®¹ï¼Œç¬¬ {attempt + 1} æ¬¡...", file=sys.stderr)
                
                # ä½¿ç”¨éšæœºè¯·æ±‚å¤´
                headers = self._get_random_headers()
                
                # æ·»åŠ éšæœºå»¶æ—¶
                time.sleep(random.uniform(0.5, 2.0))
                
                response = requests.get(url, headers=headers, timeout=20)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # å°è¯•å¤šç§æ–¹å¼æå–å†…å®¹
                content = ""
                
                # æ–¹æ³•1: æŸ¥æ‰¾å¸¸è§çš„æ–°é—»å†…å®¹å®¹å™¨
                content_selectors = [
                    '.newsContent',
                    '.article-content',
                    '.content',
                    '.main-content',
                    '.article-body',
                    '.news-body',
                    '.detail-content',
                    '.article-detail',
                    '.news-detail',
                    '.post-content',
                    '.entry-content',
                    '.story-body',
                    '.article-text',
                    '.news-text'
                ]
                
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(strip=True)
                        print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æˆåŠŸæå–å†…å®¹", file=sys.stderr)
                        break
                
                # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰æ®µè½
                if not content:
                    paragraphs = soup.find_all('p')
                    if paragraphs:
                        # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½
                        valid_paragraphs = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20]
                        if valid_paragraphs:
                            content = '\n'.join(valid_paragraphs)
                            print("ä½¿ç”¨æ®µè½æ ‡ç­¾æå–å†…å®¹", file=sys.stderr)
                
                # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨bodyå†…å®¹
                if not content:
                    body = soup.find('body')
                    if body:
                        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                        for script in body(["script", "style", "nav", "header", "footer"]):
                            script.decompose()
                        content = body.get_text(strip=True)
                        print("ä½¿ç”¨bodyæ ‡ç­¾æå–å†…å®¹", file=sys.stderr)
                
                # æ¸…ç†å†…å®¹
                if content:
                    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    content = re.sub(r'\s+', ' ', content)
                    # ç§»é™¤ä¸€äº›å¸¸è§çš„æ— ç”¨å†…å®¹
                    content = re.sub(r'åˆ†äº«åˆ°.*?å¾®ä¿¡', '', content, flags=re.DOTALL)
                    content = re.sub(r'ç›¸å…³é˜…è¯».*', '', content, flags=re.DOTALL)
                    content = re.sub(r'ç‚¹å‡»æŸ¥çœ‹.*', '', content, flags=re.DOTALL)
                    # é™åˆ¶é•¿åº¦
                    if len(content) > 2000:
                        content = content[:2000] + "..."
                    
                    print(f"æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦", file=sys.stderr)
                    return content
                else:
                    raise Exception("æ— æ³•æå–åˆ°ä»»ä½•å†…å®¹")
                    
            except Exception as e:
                print(f"ç¬¬ {attempt + 1} æ¬¡è·å–æ–°é—»å†…å®¹å¤±è´¥ {url}: {e}", file=sys.stderr)
                
                if attempt < max_retries - 1:
                    print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...", file=sys.stderr)
                    time.sleep(retry_delay)
                    retry_delay *= 1.5  # è½»å¾®æŒ‡æ•°é€€é¿
                else:
                    print(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ— æ³•è·å–æ–°é—»å†…å®¹", file=sys.stderr)
                    return ""
        
        return ""
    
    def analyze_sentiment(self, content: str, title: str = "") -> Dict:
        """
        ä½¿ç”¨DeepSeekè¿›è¡Œæƒ…æ„Ÿåˆ†æ
        
        Args:
            content: æ–°é—»å†…å®¹
            title: æ–°é—»æ ‡é¢˜
            
        Returns:
             æƒ…æ„Ÿåˆ†æç»“æœ
        """
        try:
            if not content:
                return {
                    'sentiment': 'neutral',
                    'score': 0,
                    'confidence': 0,
                    'reason': 'å†…å®¹ä¸ºç©º'
                }
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è‚¡ç¥¨ç›¸å…³æ–°é—»çš„æƒ…æ„Ÿå€¾å‘ï¼Œåˆ¤æ–­æ˜¯æ­£é¢ã€è´Ÿé¢è¿˜æ˜¯ä¸­æ€§ï¼š

æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æï¼š
1. å¯¹è‚¡ç¥¨ä»·æ ¼çš„å½±å“ï¼ˆåˆ©å¥½/åˆ©ç©º/ä¸­æ€§ï¼‰
2. å¯¹å…¬å¸åŸºæœ¬é¢çš„å½±å“
3. å¯¹è¡Œä¸šçš„å½±å“
4. å¸‚åœºæƒ…ç»ªçš„å½±å“

è¯·ç»™å‡ºï¼š
1. æƒ…æ„Ÿå€¾å‘ï¼šæ­£é¢/è´Ÿé¢/ä¸­æ€§
2. æƒ…æ„Ÿå¼ºåº¦ï¼š0-10åˆ†ï¼ˆ0æœ€è´Ÿé¢ï¼Œ10æœ€æ­£é¢ï¼‰
3. ç½®ä¿¡åº¦ï¼š0-100%
4. åˆ†æç†ç”±ï¼šç®€è¦è¯´æ˜åˆ¤æ–­ä¾æ®

è¯·ç”¨JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "sentiment": "æ­£é¢/è´Ÿé¢/ä¸­æ€§",
    "score": 0-10çš„åˆ†æ•°,
    "confidence": 0-100çš„ç½®ä¿¡åº¦,
    "reason": "åˆ†æç†ç”±"
}}
"""
            
            # è°ƒç”¨DeepSeek API - å‚è€ƒEasyMoneyNewsData.pyçš„å®ç°
            headers = {"Authorization": f"Bearer {self.deepseek_api_key}", "Content-Type": "application/json"}
            payload = {
                "model": self.deepseek_model,
                "messages": [
                    {"role": "system", "content": "ä»…è¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"},
                    {"role": "user", "content": prompt},
                ],
                "temperature": 0.3,
                "max_tokens": 500,
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0,
            }
            
            # é‡è¯•æœºåˆ¶
            max_retries = 3
            retry_delay = 2
            
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        print(f"é‡è¯•æƒ…æ„Ÿåˆ†æï¼Œç¬¬ {attempt + 1} æ¬¡...", file=sys.stderr)
                    
                    response = requests.post(self.deepseek_url, headers=headers, json=payload, timeout=30)
                    response.raise_for_status()
                    response_data = response.json()
                    response_text = response_data["choices"][0]["message"]["content"].strip()
                    
                    # å°è¯•æå–JSON
                    try:
                        # æŸ¥æ‰¾JSONéƒ¨åˆ†
                        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                        if json_match:
                            result = json.loads(json_match.group())
                            print(f"æƒ…æ„Ÿåˆ†ææˆåŠŸï¼Œæƒ…æ„Ÿå€¾å‘: {result.get('sentiment', 'unknown')}", file=sys.stderr)
                            return result
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•æ‰‹åŠ¨è§£æ
                            print("æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•æ‰‹åŠ¨è§£æ...", file=sys.stderr)
                            return self._parse_sentiment_response(response_text)
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•æ‰‹åŠ¨è§£æ...", file=sys.stderr)
                        return self._parse_sentiment_response(response_text)
                        
                except Exception as e:
                    print(f"DeepSeek APIè°ƒç”¨å¤±è´¥ ç¬¬{attempt + 1}æ¬¡: {e}", file=sys.stderr)
                    
                    if attempt < max_retries - 1:
                        print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...", file=sys.stderr)
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    else:
                        # å…œåº•ï¼šè¿”å›ä¸­æ€§ç»“æœ
                        print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œè¿”å›é»˜è®¤ä¸­æ€§ç»“æœ", file=sys.stderr)
                        return {
                            'sentiment': 'neutral',
                            'score': 5,
                            'confidence': 0,
                            'reason': f'APIè°ƒç”¨å¤±è´¥-é»˜è®¤ä¸­æ€§: {str(e)}'
                        }
                
        except Exception as e:
            print(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}", file=sys.stderr)
            return {
                'sentiment': 'neutral',
                'score': 5,
                'confidence': 0,
                'reason': f'åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _parse_sentiment_response(self, response_text: str) -> Dict:
        """
        æ‰‹åŠ¨è§£ææƒ…æ„Ÿåˆ†æå“åº”
        
        Args:
            response_text: å“åº”æ–‡æœ¬
            
        Returns:
             è§£æåçš„ç»“æœ
        """
        try:
            result = {
                'sentiment': 'neutral',
                'score': 5,
                'confidence': 50,
                'reason': response_text
            }
            
            # å°è¯•æå–æƒ…æ„Ÿå€¾å‘
            if 'æ­£é¢' in response_text or 'åˆ©å¥½' in response_text or 'ç§¯æ' in response_text:
                result['sentiment'] = 'positive'
            elif 'è´Ÿé¢' in response_text or 'åˆ©ç©º' in response_text or 'æ¶ˆæ' in response_text:
                result['sentiment'] = 'negative'
            
            # å°è¯•æå–åˆ†æ•°
            score_match = re.search(r'(\d+)(?:\s*åˆ†|\s*åˆ†)')
            if score_match:
                score = int(score_match.group(1))
                if 0 <= score <= 10:
                    result['score'] = score
            
            # å°è¯•æå–ç½®ä¿¡åº¦
            confidence_match = re.search(r'ç½®ä¿¡åº¦[ï¼š:]\s*(\d+)%?')
            if confidence_match:
                confidence = int(confidence_match.group(1))
                if 0 <= confidence <= 100:
                    result['confidence'] = confidence
            
            return result
            
        except Exception as e:
            print(f"æ‰‹åŠ¨è§£ææƒ…æ„Ÿåˆ†æå“åº”å¤±è´¥: {e}", file=sys.stderr)
            return {
                'sentiment': 'neutral',
                'score': 5,
                'confidence': 0,
                'reason': response_text
            }
    
    def analyze_all_news(self, data: Dict) -> Dict:
        """
        åˆ†ææ‰€æœ‰æ–°é—»çš„æƒ…æ„Ÿ
        
        Args:
            data: æ–°é—»æ•°æ®
            
        Returns:
             åŒ…å«æƒ…æ„Ÿåˆ†æç»“æœçš„æ•°æ®
        """
        try:
            analyzed_data = {
                'stockCode': data['stockCode'],
                'timestamp': data['timestamp'],
                'news': [],
                'notices': [],
                'reports': []
            }
            
            # åˆ†æèµ„è®¯
            print("æ­£åœ¨åˆ†æèµ„è®¯æƒ…æ„Ÿ...", file=sys.stderr)
            for news in data.get('news', []):
                print(f"åˆ†æèµ„è®¯: {news['title']}", file=sys.stderr)
                
                # è·å–å†…å®¹
                content = self.get_news_content(news['link'])
                
                # æƒ…æ„Ÿåˆ†æ
                sentiment = self.analyze_sentiment(content, news['title'])
                
                analyzed_news = news.copy()
                analyzed_news['content'] = content
                analyzed_news['sentiment'] = sentiment
                
                analyzed_data['news'].append(analyzed_news)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
            
            # åˆ†æå…¬å‘Š
            print("æ­£åœ¨åˆ†æå…¬å‘Šæƒ…æ„Ÿ...", file=sys.stderr)
            for notice in data.get('notices', []):
                print(f"åˆ†æå…¬å‘Š: {notice['title']}", file=sys.stderr)
                
                # è·å–å†…å®¹
                content = self.get_news_content(notice['link'])
                
                # æƒ…æ„Ÿåˆ†æ
                sentiment = self.analyze_sentiment(content, notice['title'])
                
                analyzed_notice = notice.copy()
                analyzed_notice['content'] = content
                analyzed_notice['sentiment'] = sentiment
                
                analyzed_data['notices'].append(analyzed_notice)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
            
            # åˆ†æç ”æŠ¥
            print("æ­£åœ¨åˆ†æç ”æŠ¥æƒ…æ„Ÿ...", file=sys.stderr)
            for report in data.get('reports', []):
                print(f"åˆ†æç ”æŠ¥: {report['title']}", file=sys.stderr)
                
                # è·å–å†…å®¹
                content = self.get_news_content(report['link'])
                
                # æƒ…æ„Ÿåˆ†æ
                sentiment = self.analyze_sentiment(content, report['title'])
                
                analyzed_report = report.copy()
                analyzed_report['content'] = content
                analyzed_report['sentiment'] = sentiment
                
                analyzed_data['reports'].append(analyzed_report)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
            
            return analyzed_data
            
        except Exception as e:
            print(f"åˆ†ææ‰€æœ‰æ–°é—»æƒ…æ„Ÿå¤±è´¥: {e}", file=sys.stderr)
            return data

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python EastMoneyStockNews.py <è‚¡ç¥¨ä»£ç > [DeepSeek_API_Key]", file=sys.stderr)
        print("ç¤ºä¾‹: python EastMoneyStockNews.py 688333", file=sys.stderr)
        sys.exit(1)
    
    stock_code = sys.argv[1]
    
    # è·å–APIå¯†é’¥
    deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
    if len(sys.argv) > 2:
        deepseek_api_key = sys.argv[2]
    
    # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œåªæµ‹è¯•æ•°æ®çˆ¬å–
    if not deepseek_api_key:
        print("âš ï¸ æœªè®¾ç½®DEEPSEEK_API_KEYï¼Œå°†åªæµ‹è¯•æ•°æ®çˆ¬å–åŠŸèƒ½", file=sys.stderr)
        deepseek_api_key = "test_key"  # ä½¿ç”¨æµ‹è¯•å¯†é’¥
    
    try:
        # åˆ›å»ºçˆ¬å–å™¨
        news_crawler = EastMoneyStockNews(deepseek_api_key)
        
        print(f"å¼€å§‹è·å–è‚¡ç¥¨ {stock_code} çš„èµ„è®¯æ•°æ®...", file=sys.stderr)
        
        # è·å–åŸå§‹æ•°æ®
        raw_data = news_crawler.get_stock_news_data(stock_code)
        if not raw_data:
            print("è·å–è‚¡ç¥¨èµ„è®¯æ•°æ®å¤±è´¥", file=sys.stderr)
            sys.exit(1)
        
        print(f"è·å–åˆ° {len(raw_data.get('news', []))} æ¡èµ„è®¯, {len(raw_data.get('notices', []))} æ¡å…¬å‘Š, {len(raw_data.get('reports', []))} æ¡ç ”æŠ¥", file=sys.stderr)
        
        # è¿‡æ»¤æœ€è¿‘ä¸€å‘¨çš„æ•°æ®
        filtered_data = news_crawler.filter_recent_week(raw_data)
        print(f"æœ€è¿‘ä¸€å‘¨: {len(filtered_data.get('news', []))} æ¡èµ„è®¯, {len(filtered_data.get('notices', []))} æ¡å…¬å‘Š, {len(filtered_data.get('reports', []))} æ¡ç ”æŠ¥", file=sys.stderr)
        
        # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œè·³è¿‡æƒ…æ„Ÿåˆ†æ
        if deepseek_api_key == "test_key":
            print("âš ï¸ è·³è¿‡æƒ…æ„Ÿåˆ†æï¼ˆéœ€è¦æœ‰æ•ˆçš„DeepSeek APIå¯†é’¥ï¼‰", file=sys.stderr)
            analyzed_data = filtered_data
        else:
            # åˆ†ææƒ…æ„Ÿ
            analyzed_data = news_crawler.analyze_all_news(filtered_data)
        
        # è¾“å‡ºç»“æœ
        print(json.dumps(analyzed_data, ensure_ascii=False, indent=2))
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
